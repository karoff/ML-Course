{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Machine Learning\n",
    "\n",
    "### List of symbols\n",
    "- $X_j^{(i)}$: input variable $j$ for data set $i$\n",
    "- $Y_j^{(i)}$: output variable $j$ for data set $i$\n",
    "- $h_{\\theta}$: hypothesis\n",
    "- $\\theta_{j}$: parameters for input variable $j$\n",
    "- $m$: number of tranings sets\n",
    "- $J(\\theta)$: Cost function\n",
    "\n",
    "## Linear Regression\n",
    "\\begin{equation}\n",
    "h_{\\theta}=\\theta_0X_0+\\theta_1X_1+...=\\theta^{\\rm T}X\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}\\left(h_{\\theta}\\left(X ^{(i)}\\right)-Y^{(i)} \\right)^2\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\left(h_{\\theta}\\left( X^{(i)}\\right)-Y^{(i)} \\right) X_j^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "## Logistic regression\n",
    "\\begin{equation}\n",
    "h_{\\theta}(X)=g\\left(\\theta^{\\rm T} X \\right),\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "g(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "J(\\theta)=\\frac{1}{m}\\left(\\sum_{i=1}^{m} Y^{(i)} {\\rm log} h_{\\theta}\\left(X^{(i)} \\right) +\\left(1-Y^{(i)} \\right){\\rm log} \\left(1-h_{\\theta}\\left(X^{(i)} \\right) \\right)\\right)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\left(h_{\\theta}\\left(X^{(i)} \\right)-Y^{(i)} \\right)X_j^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "## Gradient densent\n",
    "\\begin{equation}\n",
    "\\theta_{j}:=\\theta_j-\\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta),\n",
    "\\end{equation}\n",
    "where $\\alpha$ is the learning rate and '$:=$' means update.\n",
    "\n",
    "## Regularization\n",
    "\\begin{equation}\n",
    "J(\\theta)=\\frac{1}{m}\\left(\\sum_{i=1}^{m} Y^{(i)} {\\rm log} h_{\\theta}\\left(X^{(i)} \\right) +\\left(1-Y^{(i)} \\right){\\rm log} \\left(1-h_{\\theta}\\left(X^{(i)} \\right) \\right) + \\lambda \\sum_{j=1}^{n} \\theta_j^2\\right)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\left(h_{\\theta}\\left(X^{(i)} \\right)-Y^{(i)} \\right)X_j^{(i)}  ~{\\rm for}~ j= 1\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\left(h_{\\theta}\\left(X^{(i)} \\right)-Y^{(i)} \\right)X_j^{(i)} +\\frac{\\lambda}{m}\\theta_j ~{\\rm for}~ j\\ge 1\n",
    "\\end{equation}\n",
    "\n",
    "## Neural Network\n",
    "### List of new symbols\n",
    "- $L$: total number of layers $l$ in the network\n",
    "- $s_l$: number of units in layer $l$\n",
    "- $K$: number of output units\n",
    "- $\\theta_{ji}^{(l)}$: parameter between note $i$ in layer $s_l$ and note $j$ in layer $s_l+1$\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K\\left( Y^{(i)}_{k} {\\rm log}\\left( h_{\\theta}\\left(X^{(i)} \\right)_k \\right)+\\left( 1-Y_k^{(i)}\\right){\\rm log}\\left(1-\\left(h_{\\theta}\\left(X^{(i)} \\right)_k \\right) \\right) \\right)+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}\\left( \\theta_{ij}^{(l)} \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "## Backpropagation\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_{ij}^{\\theta}}J(\\theta)=D_{ij}^{l}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "D_{ij}^{(l)}=\\frac{1}{m}\\Delta_{ij}^{(l)} ~{\\rm for}~ j= 1\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "D_{ij}^{(l)}=\\frac{1}{m}\\Delta_{ij}^{(l)} +\\frac{\\lambda}{m}\\theta_{ij}^{(l)}~{\\rm for}~ j \\ge 1\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\Delta_{ij}^{(l)}:=\\Delta_{ij}^{(l)}+a_j\\delta^{(l+1)}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\delta^{(l)}=\\left(\\theta \\right)^{\\rm T} \\delta^{(l+1)}a^{(l)}\\left(1-a^{(l)} \\right)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\delta^{(L)}=a^{(L)}-Y\n",
    "\\end{equation}\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "### List of new symbols\n",
    "- $a$: activation map\n",
    "- $w$: kernel\n",
    "- $b$: bias\n",
    "\n",
    "### Convolution\n",
    "\\begin{equation}\n",
    "a=w^{\\rm T}X+b\n",
    "\\end{equation}\n",
    "\n",
    "### Relu\n",
    "\\begin{equation}\n",
    "f(z)={\\rm max}(0,z)\n",
    "\\end{equation}\n",
    "\n",
    "### Softmax\n",
    "\\begin{equation}\n",
    "\\sigma(z)_j=\\frac{e^{z_j}}{\\sum_{k=1}^k e^{z_k}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
